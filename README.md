# ğŸš€ CookieCutter MLServe (ONNX + FastAPI)

[![CCDS](https://img.shields.io/badge/CCDS-Project%20template-328F97?logo=cookiecutter)](https://cookiecutter-data-science.drivendata.org/)
[![Python](https://img.shields.io/badge/Python-3.12-3776AB.svg?style=flat&logo=python&logoColor=white)](https://python.org/)
[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=yellow)](https://pre-commit.com/)
[![black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://black.readthedocs.io/en/stable/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.110.0-009688?logo=fastapi)](https://fastapi.tiangolo.com/)
[![ONNX](https://img.shields.io/badge/ONNX-1.16.0-005CED?logo=onnx)](https://onnx.ai/)
[![Docker](https://img.shields.io/badge/Docker-Ready-2496ED?logo=docker&logoColor=white)](https://www.docker.com/)
[![Pydantic](https://img.shields.io/badge/Pydantic-2.9.2-0984E3?logo=pydantic&logoColor=white)](https://docs.pydantic.dev/)

This repository provides a **production-ready starter template** for deploying machine learning models using **[ONNX](https://onnx.ai/)** as the primary model format.
Itâ€™s designed to help you get from a trained model to a running, API-served inference service **fast**, while keeping all the essentials for maintainability, observability, and scalability.

---

## âœ¨ Features

- **ONNX Model Serving** â€” Load and serve models in the ONNX format for portable, high-performance inference.
- **FastAPI** â€” High-speed, easy-to-use API server for inference requests.
- **Pydantic** â€” Strict request/response validation for reliable data handling.
- **Structured Logging** â€” Write meaningful, structured logs to files (request logs, system logs, errors).
- **User Request Tracking** â€” Automatically log each API request for monitoring and auditing.
- **Model Version Tracking** â€” Store and expose the deployed model version.
- **Dataset Version Tracking** â€” Keep track of which dataset version was used to train the deployed model.
- **Health Check Endpoint** â€” Quickly verify service readiness and liveness.
- **Configurable Settings** â€” Use environment variables for flexible deployments.
- **Docker-Ready** â€” Preconfigured `Dockerfile` for containerized deployment.

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py           # FastAPI entry point
â”‚   â”œâ”€â”€ config.py         # Configuration & environment variables
â”‚   â”œâ”€â”€ logger.py         # Logging setup
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ model.onnx    # Example ONNX model
â”‚   â”œâ”€â”€ schemas/          # Pydantic request/response schemas
â”‚   â””â”€â”€ utils/            # Utility functions (e.g., version tracking)
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ requests.log      # All incoming requests
â”‚   â””â”€â”€ system.log        # System and error logs
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

---

## âš¡ Getting Started

### 1ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ Set Environment Variables
Copy the example environment file:
```bash
cp .env.example .env
```
Update `.env` with:
- `MODEL_PATH` â€” Path to your ONNX model.
- `MODEL_VERSION` â€” Version of your deployed model.
- `DATASET_VERSION` â€” Version of the dataset used to train the model.
- `LOG_DIR` â€” Directory where logs should be stored.

### 3ï¸âƒ£ Run the Server
```bash
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```
The API will be available at:
â¡ï¸ `http://localhost:8000`

---

## ğŸ›  Example API Endpoints

| Method | Endpoint         | Description                          |
|--------|------------------|--------------------------------------|
| GET    | `/health`        | Check if the service is running.     |
| POST   | `/predict`       | Run inference on input data.         |
| GET    | `/metadata`      | Retrieve model and dataset versions. |

Example prediction request:
```json
POST /predict
Content-Type: application/json

{
  "feature1": 0.45,
  "feature2": 1.23,
  "feature3": 3.14
}
```

Example response:
```json
{
  "prediction": "class_A",
  "confidence": 0.92
}
```

---

## ğŸ“ Logging

This repository uses **structured logging** for better observability:
- `logs/system.log` â€” Application startup, errors, and general info.
- `logs/requests.log` â€” Each incoming request, including timestamp and parameters.

Example system log:
```
2025-08-08 10:15:23 [INFO] Model loaded: model_v1.2.0.onnx
```

Example request log:
```
2025-08-08 10:16:05 [REQUEST] /predict - {"feature1":0.45,"feature2":1.23}
```

---

## ğŸ“¦ Deployment

### Using Docker
```bash
docker build -t ml-deployment-starter .
docker run -p 8000:8000 ml-deployment-starter
```

---

## ğŸ›¡ Best Practices Included

âœ… Versioned model & dataset metadata
âœ… Validated API contracts using Pydantic
âœ… Centralized configuration management
âœ… Separation of concerns in code structure
âœ… Logging for debugging and monitoring

---

## ğŸ“œ License

[LICENSE](LICENSE)

---

**ğŸ’¡ Tip:** This repo is a starting point â€” extend it with authentication, monitoring dashboards, or CI/CD integration for full production readiness.

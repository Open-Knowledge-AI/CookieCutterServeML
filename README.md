# ğŸš€ Serve-ML â€“ Cookiecutter Starter for Serving Machine Learning Models

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/release/python-3110/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-009688?logo=fastapi)](https://fastapi.tiangolo.com/)
[![Docker Ready](https://img.shields.io/badge/Docker-ready-2496ED?logo=docker)](https://www.docker.com/)
[![Pre-commit Hooks](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit)](https://pre-commit.com/)
[![Code Style: Black](https://img.shields.io/badge/code%20style-black-black.svg)](https://github.com/psf/black)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)

---

A **production-ready starter template** for serving machine learning models with **FastAPI**.
This repo gives you a **plug-and-play ML API server** with best practices for:

- ğŸ“ Structured logging
- ğŸ³ Containerization
- ğŸ§¹ Code quality (linting, formatting, pre-commit hooks)
- ğŸ“¦ Registry of your models

Whether youâ€™re just prototyping or preparing for production, this template saves you the backend setup hassle.

---

## ğŸ“‚ Project Structure

```
src/
â”œâ”€â”€ Dockerfile                  # Container setup
â”œâ”€â”€ Makefile                    # Automation (lint, run, etc.)
â”œâ”€â”€ pyproject.toml              # Metadata + dev tooling
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .pre-commit-config.yaml     # Pre-commit hooks
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ config.py               # Logging, env, paths
â”‚   â”œâ”€â”€ main.py                 # FastAPI app entrypoint
â”‚   â”œâ”€â”€ api/                    # API routers
â”‚   â”‚   â”œâ”€â”€ health.py           # /, /health, /version
â”‚   â”‚   â”œâ”€â”€ predict.py          # /predict, /predict-batch
â”‚   â”‚   â””â”€â”€ registry.py         # /registry endpoints
â”‚   â””â”€â”€ core/                   # Middleware + registry backends
â”‚       â”œâ”€â”€ middleware.py
â”‚       â””â”€â”€ registry.py
â”œâ”€â”€ models/                     # ML model files live here
â”‚   â””â”€â”€{dataset_version}/       # e.g., imagenet_v1/
â”‚       â””â”€â”€{architecture}/      # e.g., resnet50/
â”‚           â””â”€â”€{model_version}/ # e.g., v1/
â”‚               â””â”€â”€ model files # e.g. model.onnx, model.json
â”œâ”€â”€ assets/                     # Static assets (e.g., favicon)
â””â”€â”€ logs/                       # Rotating system & request logs
````

---

## âš¡ Features

* âœ… **FastAPI REST API** â€“ blazing fast, async-ready
* âœ… **Model registry** â€“ discover models via `/registry`
* âœ… **Inference endpoints** â€“ `/predict` and `/predict-batch`
* âœ… **Health & version endpoints** â€“ `/health`, `/version`
* âœ… **Structured logging** â€“ with sensitive field filtering
* âœ… **Pre-commit hooks** â€“ keep code clean before commits
* âœ… **Dockerized** â€“ deploy anywhere
* âœ… **Makefile automation** â€“ simple dev UX

---

## ğŸ› ï¸ Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/your-org/open-knowledge-ai-cookiecutterserveml.git
cd open-knowledge-ai-cookiecutterserveml
```

### 2. Setup with Make

```bash
make env   # create venv, install deps, setup hooks
make run   # start FastAPI at http://0.0.0.0:8000
```

### 3. Run with Docker

```bash
docker build -t serve-ml .
docker run -p 8000:8000 serve-ml
```

---

## ğŸŒ API Endpoints

| Endpoint                             | Method | Description                           |
| ------------------------------------ | ------ | ------------------------------------- |
| `/`                                  | GET    | Welcome message                       |
| `/health`                            | GET    | Health check (returns `{status: ok}`) |
| `/version`                           | GET    | App version from `pyproject.toml`     |
| `/predict`                           | POST   | Predict on a single file              |
| `/predict-batch`                     | POST   | Predict on multiple files             |
| `/registry`                          | GET    | List all available models             |
| `/registry/{dataset}/{arch}/{model}` | GET    | Get details for a specific model      |

ğŸ” Example request:

```bash
curl -X POST "http://localhost:8000/predict" \
  -F "model_name=my_model" \
  -F "input_data=@sample_input.json"
```

---

## ğŸ§‘â€ğŸ’» Development

### Formatting & Linting

```bash
make format   # auto-format with black
make lint     # check style with flake8
```

### Cleaning Build Files

```bash
make clean
```

### Pre-commit Hooks

```bash
make setup_hooks
```

---

## ğŸ“Š Logging

* Request logs â†’ `logs/requests.log`
* System logs â†’ `logs/system.log`

Logs are:

* Structured (readable + JSON ready)
* Rotated & compressed automatically
* Sensitive fields (`password`, `token`, etc.) masked

---

## ğŸ“¦ Requirements

* Python **3.11+**
* [FastAPI](https://fastapi.tiangolo.com/)
* [Uvicorn](https://www.uvicorn.org/)
* [Docker](https://www.docker.com/) (optional)

---

## ğŸš€ Roadmap

* [x] FastAPI serving template
* [x] Model registry (local filesystem)
* [ ] ONNX / TensorFlow / PyTorch inference helpers
* [ ] Authentication & API keys
* [ ] Async batch processing
* [ ] Multi-backend registry (S3, MLflow)
* [ ] Deployment templates (K8s, AWS, GCP)

---

## ğŸ¤ Contributing

Contributions are welcome ğŸ‰
Fork, open an issue, or submit a PR. Please run:

```bash
make format lint
```

before committing.

---

## ğŸ“œ License

This project is licensed under the **Apache 2.0 License** â€“ see the [LICENSE](LICENSE) file for details.

---
